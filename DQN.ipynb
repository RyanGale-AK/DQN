{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorwatch as tw\n",
    "\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import collections\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "NOTEBOOK_MODE = True\n",
    "if NOTEBOOK_MODE:\n",
    "    from tqdm.notebook import tqdm\n",
    "else:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "\n",
    "env = gym.make('Pong-v0').unwrapped\n",
    "if NOTEBOOK_MODE:\n",
    "    # set up matplotlib to open viewing window\n",
    "    is_ipython = 'inline' in matplotlib.get_backend()\n",
    "    if is_ipython:\n",
    "        from IPython import display\n",
    "\n",
    "    plt.ion()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarpFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env, width=84, height=84, grayscale=True, dict_space_key=None):\n",
    "        \"\"\"\n",
    "        Warp frames to 84x84 as done in the Nature paper and later work.\n",
    "        If the environment uses dictionary observations, `dict_space_key` can be specified which indicates which\n",
    "        observation should be warped.\n",
    "        \"\"\"\n",
    "        super().__init__(env)\n",
    "        self._width = width\n",
    "        self._height = height\n",
    "        self._grayscale = grayscale\n",
    "        self._key = dict_space_key\n",
    "        if self._grayscale:\n",
    "            num_colors = 1\n",
    "        else:\n",
    "            num_colors = 3\n",
    "\n",
    "        new_space = gym.spaces.Box(\n",
    "            low=0,\n",
    "            high=255,\n",
    "            shape=(self._height, self._width, num_colors),\n",
    "            dtype=np.uint8,\n",
    "        )\n",
    "        if self._key is None:\n",
    "            original_space = self.observation_space\n",
    "            self.observation_space = new_space\n",
    "        else:\n",
    "            original_space = self.observation_space.spaces[self._key]\n",
    "            self.observation_space.spaces[self._key] = new_space\n",
    "        assert original_space.dtype == np.uint8 and len(original_space.shape) == 3\n",
    "\n",
    "    def observation(self, obs):\n",
    "        if self._key is None:\n",
    "            frame = obs\n",
    "        else:\n",
    "            frame = obs[self._key]\n",
    "\n",
    "        if self._grayscale:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        frame = cv2.resize(\n",
    "            frame, (self._width, self._height), interpolation=cv2.INTER_AREA\n",
    "        )\n",
    "        if self._grayscale:\n",
    "            frame = np.expand_dims(frame, -1)\n",
    "\n",
    "        if self._key is None:\n",
    "            obs = frame\n",
    "        else:\n",
    "            obs = obs.copy()\n",
    "            obs[self._key] = frame\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.1.2) /io/opencv/modules/imgproc/src/color.simd_helpers.hpp:92: error: (-2:Unspecified error) in function 'cv::impl::{anonymous}::CvtHelper<VScn, VDcn, VDepth, sizePolicy>::CvtHelper(cv::InputArray, cv::OutputArray, int) [with VScn = cv::impl::{anonymous}::Set<3, 4>; VDcn = cv::impl::{anonymous}::Set<1>; VDepth = cv::impl::{anonymous}::Set<0, 2, 5>; cv::impl::{anonymous}::SizePolicy sizePolicy = (cv::impl::<unnamed>::SizePolicy)2u; cv::InputArray = const cv::_InputArray&; cv::OutputArray = const cv::_OutputArray&]'\n> Invalid number of channels in input image:\n>     'VScn::contains(scn)'\n> where\n>     'scn' is 1\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5703e3d2e662>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mWarpFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-f7f68eaf6b9c>\u001b[0m in \u001b[0;36mobservation\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grayscale\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_RGB2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         frame = cv2.resize(\n\u001b[1;32m     41\u001b[0m             \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINTER_AREA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.1.2) /io/opencv/modules/imgproc/src/color.simd_helpers.hpp:92: error: (-2:Unspecified error) in function 'cv::impl::{anonymous}::CvtHelper<VScn, VDcn, VDepth, sizePolicy>::CvtHelper(cv::InputArray, cv::OutputArray, int) [with VScn = cv::impl::{anonymous}::Set<3, 4>; VDcn = cv::impl::{anonymous}::Set<1>; VDepth = cv::impl::{anonymous}::Set<0, 2, 5>; cv::impl::{anonymous}::SizePolicy sizePolicy = (cv::impl::<unnamed>::SizePolicy)2u; cv::InputArray = const cv::_InputArray&; cv::OutputArray = const cv::_OutputArray&]'\n> Invalid number of channels in input image:\n>     'VScn::contains(scn)'\n> where\n>     'scn' is 1\n"
     ]
    }
   ],
   "source": [
    "WarpFrame(env).observation(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "watcher = tw.Watcher() # tensorwatch watcher (Microsoft Tensorwatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(64, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "def get_screen():\n",
    "    # convert to channel,h,w dimensions\n",
    "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "    \n",
    "    # erase background\n",
    "    screen[screen == 72] = 0 \n",
    "    screen[screen == 74] = 0 \n",
    "    screen[screen == 144] = 0 \n",
    "    screen[screen != 0] = 213\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    \n",
    "    screen = torch.from_numpy(screen)\n",
    "    \n",
    "    # convert to batch,channel,h,w dimensions\n",
    "    return resize(screen).unsqueeze(0).to(device)\n",
    "\n",
    "if NOTEBOOK_MODE:\n",
    "    env.reset()\n",
    "    # run game for a bit to load the ball and opponent paddle\n",
    "    for i in range(50):\n",
    "        env.step(0)\n",
    "    plt.figure()\n",
    "    plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
    "               interpolation='none')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0005\n",
    "gamma = 0.98\n",
    "buffer_limit = 50000\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_screen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-53fb0fc4d6a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_screen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'get_screen' is not defined"
     ]
    }
   ],
   "source": [
    "get_screen().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "store previous sequence-action pairs to decorrelate temporal locality\n",
    "transitions are composed of state, action, reward, next_state, and done_mask\n",
    "\"\"\"\n",
    "x_tmp = []\n",
    "class ReplayBuffer():\n",
    "    def __init__(self):\n",
    "        self.buffer = collections.deque(maxlen=buffer_limit)\n",
    "    \n",
    "    def put(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "        \n",
    "    def sample(self, n):\n",
    "        mini_batch = random.sample(self.buffer, n)\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
    "        \n",
    "        for transition in mini_batch:\n",
    "            s, a, r, s_prime, done_mask = transition\n",
    "            s_lst.append(s)\n",
    "            a_lst.append([a])\n",
    "            r_lst.append([r])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            done_mask_lst.append([done_mask])\n",
    "            \n",
    "        return  torch.cat(s_lst).to(device), \\\n",
    "                torch.tensor(a_lst).to(device), \\\n",
    "                torch.tensor(r_lst).to(device),\\\n",
    "                torch.cat(s_prime_lst).to(device), \\\n",
    "                torch.tensor(done_mask_lst).to(device)\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, h, w = get_screen().shape\n",
    "\n",
    "class Qnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Qnet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=8, stride=4)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        def conv2d_size_out(size, kernel_size = 4, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w, 8, 4), 4, 2), 3, 1)\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h, 8, 4), 4, 2), 3, 1)\n",
    "        linear_input_size = convw * convh * 64\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(linear_input_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))\n",
    "    \n",
    "    # action is either random or max probability estimated by Qnet\n",
    "    def sample_action(self, obs, epsilon):\n",
    "        out = self.forward(obs) # don't need if random action \n",
    "        coin = random.random()\n",
    "        if coin < epsilon:\n",
    "            return random.randint(0,1)\n",
    "        else:\n",
    "            return out.argmax().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(q, q_target, memory, optimizer):\n",
    "    s,a,r,s_prime,done_mask = memory.sample(batch_size)\n",
    "\n",
    "    q_out = q(s)\n",
    "    # collect output from the chosen action dimension\n",
    "    q_a = q_out.gather(1,a) \n",
    "\n",
    "    # most reward we get in next state s_prime\n",
    "    max_q_prime = q_target(s_prime).max(1)[0].unsqueeze(1)\n",
    "    target = r + gamma * max_q_prime * done_mask\n",
    "    # how much is our policy different from the true target \n",
    "    loss = F.smooth_l1_loss(q_a, target)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(num_episodes, watcher):\n",
    "    env = gym.make('Pong-v0')\n",
    "    q = Qnet().to(device)\n",
    "    q_target = Qnet().to(device)\n",
    "    q_target.load_state_dict(q.state_dict())\n",
    "    memory = ReplayBuffer()\n",
    "    \n",
    "    save_interval = 250\n",
    "    print_interval = 1\n",
    "    score = 0.0\n",
    "    optimizer = optim.Adam(q.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for episode in tqdm(range(1,num_episodes+1)):\n",
    "        # anneal 8% to 1% over training\n",
    "        epsilon = max(0.01, 0.08 - 0.01*(episode/200))\n",
    "        env.reset()\n",
    "        current_s = get_screen()\n",
    "        done = False\n",
    "        last_s = get_screen()\n",
    "        current_s = get_screen()\n",
    "        s = last_s - current_s\n",
    "        episode_score = 0\n",
    "        while not done:\n",
    "            a = q.sample_action(s, epsilon)\n",
    "            # first variable would be s_prime but we have get_screen\n",
    "            _, r, done, info = env.step(a + 2)\n",
    "            last_s = current_s\n",
    "            current_s = get_screen()\n",
    "            s_prime = last_s - current_s\n",
    "            \n",
    "            done_mask = 0.0 if done else 1.0\n",
    "            memory.put((s,a,r/100.,s_prime,done_mask))\n",
    "            s = s_prime\n",
    "            \n",
    "            score += r\n",
    "            episode_score += r\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            if memory.size()>2000:\n",
    "                train(q, q_target, memory, optimizer)\n",
    "        watcher.observe(score = episode_score)\n",
    "        if episode%print_interval==0 and episode!=0:\n",
    "            q_target.load_state_dict(q.state_dict())\n",
    "            print(\"n_episode : {}, Average Score : {:.1f}, Episode Score : {:.1f}, n_buffer : {}, eps : {:.1f}%\".format(\n",
    "                episode, score/episode, episode_score, memory.size(), epsilon*100))\n",
    "            \n",
    "        if episode%save_interval==0 and episode!=0:\n",
    "            # save model weights \n",
    "            torch.save(q_target.state_dict(), 'checkpoints/target_bot_%s.pt' % episode)\n",
    "            torch.save(q.state_dict(), 'checkpoints/policy_bot_%s.pt' % episode)\n",
    "    # save final model weights \n",
    "    torch.save(q_target.state_dict(), 'checkpoints/target_bot_final.pt')\n",
    "    torch.save(q.state_dict(), 'checkpoints/policy_bot_final.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(4, watcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = Qnet().to(device)\n",
    "q.load_state_dict(torch.load('checkpoints/target_bot_final.pt'))\n",
    "q.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record trained agent gameplay\n",
    "\n",
    "frames = []\n",
    "\n",
    "env.reset()\n",
    "current_s = get_screen()\n",
    "done = False\n",
    "last_s = get_screen()\n",
    "current_s = get_screen()\n",
    "s = last_s - current_s\n",
    "epsilon = 0.0\n",
    "while not done:\n",
    "    a = q.sample_action(s, epsilon) + 2\n",
    "    \n",
    "    # use environment's frame instead of preprocessed get_screen()\n",
    "    next_frame, _, done, info = env.step(a)\n",
    "    frames.append(next_frame)\n",
    "    last_s = current_s\n",
    "    current_s = get_screen()\n",
    "    s_prime = last_s - current_s\n",
    "\n",
    "    done_mask = 0.0 if done else 1.0\n",
    "    s = s_prime\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save game to video \n",
    "height, width = frames[0].shape[:2] \n",
    "\n",
    "writer = cv2.VideoWriter_fourcc('M','J','P','G')\n",
    "fps = 30\n",
    "video_file = 'playback.avi'\n",
    "out = cv2.VideoWriter(video_file, writer, fps, (width,height))\n",
    "for frame in frames:\n",
    "    out.write(frame)\n",
    "\n",
    "out.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
