{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorwatch as tw\n",
    "\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import collections\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from wrappers import make_env\n",
    "\n",
    "NOTEBOOK_MODE = True\n",
    "if NOTEBOOK_MODE:\n",
    "    from tqdm.notebook import tqdm\n",
    "else:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "\n",
    "env = gym.make('Pong-v0').unwrapped\n",
    "if NOTEBOOK_MODE:\n",
    "    # set up matplotlib to open viewing window\n",
    "    is_ipython = 'inline' in matplotlib.get_backend()\n",
    "    if is_ipython:\n",
    "        from IPython import display\n",
    "\n",
    "    plt.ion()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FrameStack<WarpFrame<FireResetEnv<MaxAndSkipEnv<NoopResetEnv<EpisodicLifeEnv<FrameStack<WarpFrame<FireResetEnv<MaxAndSkipEnv<NoopResetEnv<EpisodicLifeEnv<AtariEnv<Pong-v0>>>>>>>>>>>>>>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = make_env(env)\n",
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "watcher = tw.Watcher() # tensorwatch watcher (Microsoft Tensorwatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(64, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "def get_screen():\n",
    "    # convert to channel,h,w dimensions\n",
    "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "    \n",
    "    # erase background\n",
    "    screen[screen == 72] = 0 \n",
    "    screen[screen == 74] = 0 \n",
    "    screen[screen == 144] = 0 \n",
    "    screen[screen != 0] = 213\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    \n",
    "    screen = torch.from_numpy(screen)\n",
    "    \n",
    "    # convert to batch,channel,h,w dimensions\n",
    "    return resize(screen).unsqueeze(0).to(device)\n",
    "\n",
    "if NOTEBOOK_MODE:\n",
    "    env.reset()\n",
    "    # run game for a bit to load the ball and opponent paddle\n",
    "    for i in range(50):\n",
    "        env.step(0)\n",
    "    plt.figure()\n",
    "    plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
    "               interpolation='none')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0005\n",
    "gamma = 0.98\n",
    "buffer_limit = 50000\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 84, 64])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_screen().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1f8cc34286fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#import torch.nn as nn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mQnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "#import torch.nn as nn\n",
    "\n",
    "class Qnet(nn.Module):\n",
    "    def __init__(self, h, w, n_actions):\n",
    "        super(Qnet, self).__init__()\n",
    "        self.n_actions = n_actions\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=8, stride=4)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        def conv2d_size_out(size, kernel_size = 4, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w, 8, 4), 4, 2), 3, 1)\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h, 8, 4), 4, 2), 3, 1)\n",
    "        linear_input_size = convw * convh * 64\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(linear_input_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))\n",
    "    \n",
    "    # action is either random or max probability estimated by Qnet\n",
    "    def sample_action(self, obs, epsilon):\n",
    "        out = self.forward(obs) # don't need if random action \n",
    "        coin = random.random()\n",
    "        if coin < epsilon:\n",
    "            return random.randint(0, self.n_actions-1)\n",
    "        else:\n",
    "            return out.argmax().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "store previous sequence-action pairs to decorrelate temporal locality\n",
    "transitions are composed of state, action, reward, next_state, and done_mask\n",
    "\"\"\"\n",
    "x_tmp = []\n",
    "class ReplayBuffer():\n",
    "    def __init__(self):\n",
    "        self.buffer = collections.deque(maxlen=buffer_limit)\n",
    "    \n",
    "    def put(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "        \n",
    "    def sample(self, n):\n",
    "        mini_batch = random.sample(self.buffer, n)\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
    "        \n",
    "        for transition in mini_batch:\n",
    "            s, a, r, s_prime, done_mask = transition\n",
    "            s_lst.append(s)\n",
    "            a_lst.append([a])\n",
    "            r_lst.append([r])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            done_mask_lst.append([done_mask])\n",
    "            \n",
    "        return  torch.cat(s_lst).to(device), \\\n",
    "                torch.tensor(a_lst).to(device), \\\n",
    "                torch.tensor(r_lst).to(device),\\\n",
    "                torch.cat(s_prime_lst).to(device), \\\n",
    "                torch.tensor(done_mask_lst).to(device)\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, h, w = get_screen().shape\n",
    "\n",
    "class Qnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Qnet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=8, stride=4)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        def conv2d_size_out(size, kernel_size = 4, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w, 8, 4), 4, 2), 3, 1)\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h, 8, 4), 4, 2), 3, 1)\n",
    "        linear_input_size = convw * convh * 64\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(linear_input_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))\n",
    "    \n",
    "    # action is either random or max probability estimated by Qnet\n",
    "    def sample_action(self, obs, epsilon):\n",
    "        out = self.forward(obs) # don't need if random action \n",
    "        coin = random.random()\n",
    "        if coin < epsilon:\n",
    "            return random.randint(0,1)\n",
    "        else:\n",
    "            return out.argmax().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(q, q_target, memory, optimizer):\n",
    "    s,a,r,s_prime,done_mask = memory.sample(batch_size)\n",
    "\n",
    "    q_out = q(s)\n",
    "    # collect output from the chosen action dimension\n",
    "    q_a = q_out.gather(1,a) \n",
    "\n",
    "    # most reward we get in next state s_prime\n",
    "    max_q_prime = q_target(s_prime).max(1)[0].unsqueeze(1)\n",
    "    target = r + gamma * max_q_prime * done_mask\n",
    "    # how much is our policy different from the true target \n",
    "    loss = F.smooth_l1_loss(q_a, target)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_screen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-129-50c16b2a41ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mget_screen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'get_screen' is not defined"
     ]
    }
   ],
   "source": [
    "obs, reward, done, info = env.step(0)\n",
    "get_screen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(num_episodes, watcher):\n",
    "    env = gym.make('Pong-v0')\n",
    "    q = Qnet().to(device)\n",
    "    q_target = Qnet().to(device)\n",
    "    q_target.load_state_dict(q.state_dict())\n",
    "    memory = ReplayBuffer()\n",
    "    \n",
    "    save_interval = 250\n",
    "    print_interval = 1\n",
    "    score = 0.0\n",
    "    optimizer = optim.Adam(q.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for episode in tqdm(range(1,num_episodes+1)):\n",
    "        # anneal 8% to 1% over training\n",
    "        epsilon = max(0.01, 0.08 - 0.01*(episode/200))\n",
    "        env.reset()\n",
    "        current_s = get_screen()\n",
    "        done = False\n",
    "        last_s = get_screen()\n",
    "        current_s = get_screen()\n",
    "        s = last_s - current_s\n",
    "        episode_score = 0\n",
    "        while not done:\n",
    "            a = q.sample_action(s, epsilon)\n",
    "            # first variable would be s_prime but we have get_screen\n",
    "            _, r, done, info = env.step(a + 2)\n",
    "            last_s = current_s\n",
    "            current_s = get_screen()\n",
    "            s_prime = last_s - current_s\n",
    "            \n",
    "            done_mask = 0.0 if done else 1.0\n",
    "            memory.put((s,a,r/100.,s_prime,done_mask))\n",
    "            s = s_prime\n",
    "            \n",
    "            score += r\n",
    "            episode_score += r\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            if memory.size()>2000:\n",
    "                train(q, q_target, memory, optimizer)\n",
    "        watcher.observe(score = episode_score)\n",
    "        if episode%print_interval==0 and episode!=0:\n",
    "            q_target.load_state_dict(q.state_dict())\n",
    "            print(\"n_episode : {}, Average Score : {:.1f}, Episode Score : {:.1f}, n_buffer : {}, eps : {:.1f}%\".format(\n",
    "                episode, score/episode, episode_score, memory.size(), epsilon*100))\n",
    "            \n",
    "        if episode%save_interval==0 and episode!=0:\n",
    "            # save model weights \n",
    "            torch.save(q_target.state_dict(), 'checkpoints/target_bot_%s.pt' % episode)\n",
    "            torch.save(q.state_dict(), 'checkpoints/policy_bot_%s.pt' % episode)\n",
    "    # save final model weights \n",
    "    torch.save(q_target.state_dict(), 'checkpoints/target_bot_final.pt')\n",
    "    torch.save(q.state_dict(), 'checkpoints/policy_bot_final.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(4, watcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = Qnet().to(device)\n",
    "q.load_state_dict(torch.load('checkpoints/target_bot_final.pt'))\n",
    "q.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record trained agent gameplay\n",
    "\n",
    "frames = []\n",
    "\n",
    "env.reset()\n",
    "current_s = get_screen()\n",
    "done = False\n",
    "last_s = get_screen()\n",
    "current_s = get_screen()\n",
    "s = last_s - current_s\n",
    "epsilon = 0.0\n",
    "while not done:\n",
    "    a = q.sample_action(s, epsilon) + 2\n",
    "    \n",
    "    # use environment's frame instead of preprocessed get_screen()\n",
    "    next_frame, _, done, info = env.step(a)\n",
    "    frames.append(next_frame)\n",
    "    last_s = current_s\n",
    "    current_s = get_screen()\n",
    "    s_prime = last_s - current_s\n",
    "\n",
    "    done_mask = 0.0 if done else 1.0\n",
    "    s = s_prime\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save game to video \n",
    "height, width = frames[0].shape[:2] \n",
    "\n",
    "writer = cv2.VideoWriter_fourcc('M','J','P','G')\n",
    "fps = 30\n",
    "video_file = 'playback.avi'\n",
    "out = cv2.VideoWriter(video_file, writer, fps, (width,height))\n",
    "for frame in frames:\n",
    "    out.write(frame)\n",
    "\n",
    "out.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
