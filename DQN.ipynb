{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorwatch as tw\n",
    "\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import collections\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from wrappers import make_env\n",
    "\n",
    "NOTEBOOK_MODE = True\n",
    "if NOTEBOOK_MODE:\n",
    "    from tqdm.notebook import tqdm\n",
    "else:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "\n",
    "env = gym.make('Pong-v0').unwrapped\n",
    "if NOTEBOOK_MODE:\n",
    "    # set up matplotlib to open viewing window\n",
    "    is_ipython = 'inline' in matplotlib.get_backend()\n",
    "    if is_ipython:\n",
    "        from IPython import display\n",
    "\n",
    "    plt.ion()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import argparse\n",
    "import math\n",
    "import os\n",
    "import tensorwatch as tw\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# from apex import amp # playing around with mixed-precision training\n",
    "\n",
    "# Local Imports\n",
    "from models import Qnet\n",
    "from wrappers import make_env\n",
    "from memory import ReplayBuffer\n",
    "from helpers import saveTrainedGameplay, get_state\n",
    "from settings import device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(64, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "def get_screen():\n",
    "    # convert to channel,h,w dimensions\n",
    "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "    \n",
    "    # erase background\n",
    "    screen[screen == 72] = 0 \n",
    "    screen[screen == 74] = 0 \n",
    "    screen[screen == 144] = 0 \n",
    "    screen[screen != 0] = 213\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    \n",
    "    screen = torch.from_numpy(screen)\n",
    "    \n",
    "    # convert to batch,channel,h,w dimensions\n",
    "    return resize(screen).unsqueeze(0).to(device)\n",
    "\n",
    "if NOTEBOOK_MODE:\n",
    "    env.reset()\n",
    "    # run game for a bit to load the ball and opponent paddle\n",
    "    for i in range(50):\n",
    "        env.step(0)\n",
    "    plt.figure()\n",
    "    plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
    "               interpolation='none')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76111868862c4e69a4ff3b8621066158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_episode : 1, Total Frames : 1660, Average Score : 21.0, Episode Score : 21.0, Best Score : 21.0, n_buffer : 1660, eps : 1.0%\n",
      "n_episode : 2, Total Frames : 3386, Average Score : 21.0, Episode Score : 21.0, Best Score : 21.0, n_buffer : 3386, eps : 1.0%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-118-c3eb1c1b2aad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mdone_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/RL/helpers.py\u001b[0m in \u001b[0;36mget_state\u001b[0;34m(obs)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Optimizes our training policy by computing the Huber Loss between our minibatch of samples and the maximum possible reward for the next state(s)\n",
    "    Huber Loss here is defined as:\n",
    "    loss(x,y) = \\frac{1}{n}\\sum{z_i}, where z_i = 0.5(x_i-y_i)^2; if |x_i - y_i| < 1 or \n",
    "                                                = |x_i - y_i| - 0.5; otherwise\n",
    "'''\n",
    "def train(q, q_target, memory, optimizer, batch_size, gamma):\n",
    "    s,a,r,s_prime,done_mask = memory.sample(batch_size)\n",
    "    \n",
    "    q_out = q(s)\n",
    "    # collect output from the chosen action dimension\n",
    "    q_a = q_out.gather(1,a) \n",
    "    \n",
    "    # most reward we get in next state s_prime\n",
    "    max_q_prime = q_target(s_prime).max(1)[0].unsqueeze(1)\n",
    "    target = r + gamma * max_q_prime * done_mask\n",
    "\n",
    "    # how much is our policy different from the true target \n",
    "    loss = F.smooth_l1_loss(q_a, target)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    #with amp.scale_loss(loss, optimizer) as scaled_loss: # playing around with mixed-precision training\n",
    "    \t#scaled_loss.backward()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "#def main(num_episodes, episode_start = 1, saved_model = None, save_loc = 'checkpoints/tmp/'):\n",
    "#watcher = tw.Watcher()\n",
    "num_episodes = 100\n",
    "episode_start = 1\n",
    "saved_model = 'checkpoints/4channel/target_bot_1500.pt'\n",
    "save_loc = None\n",
    "# Model parameters \n",
    "learning_rate = 1e-4 # 0.0001 matches paper\n",
    "gamma = 0.98\n",
    "buffer_limit = 10 ** 5 # paper uses 1M last frames, but this is expensive, so we try 10x less\n",
    "batch_size = 32\n",
    "\n",
    "# Epsilon Decay Parameters\n",
    "eps_start = 1\n",
    "eps_end = 0.01\n",
    "decay_factor = 10 ** 5\n",
    "\n",
    "epsilon_decay = lambda x: eps_end + (eps_start - eps_end) * \\\n",
    "    math.exp(-1. * x / decay_factor)\n",
    "\n",
    "env = gym.make('PongNoFrameskip-v4')\n",
    "env = make_env(env)\n",
    "h, w = 84, 84\n",
    "\n",
    "\n",
    "# Initialize the policy (q) network, target network, and experience replay buffer\n",
    "q = Qnet(h,w, in_channels = 4, n_actions = 4).to(device)\n",
    "q_target = Qnet(h,w, in_channels = 4, n_actions = 4).to(device)\n",
    "memory = ReplayBuffer(buffer_limit = buffer_limit)\n",
    "\n",
    "if saved_model:\n",
    "    # Set epsilon decay function to eps_end\n",
    "    epsilon_decay = lambda x: eps_end\n",
    "    # Load pretrained model\n",
    "    q.load_state_dict(torch.load(saved_model))\n",
    "\n",
    "# Load policy weights into target network\n",
    "q_target.load_state_dict(q.state_dict())\n",
    "optimizer = optim.Adam(q.parameters(), lr=learning_rate)\n",
    "\n",
    "save_interval = 250\n",
    "print_interval = 1\n",
    "update_target_interval = 1000 # every 1000 frames\n",
    "score = 0.0\n",
    "\n",
    "\n",
    "#[q, q_target], optimizer = amp.initialize([q, q_target], optimizer, opt_level=\"O1\") #playing around with mixed-precision training\n",
    "total_frames = 0\n",
    "best_episode_score = -100\n",
    "for episode in tqdm(range(episode_start,episode_start + num_episodes)):\n",
    "    # anneal 100% to 1% over training\n",
    "    epsilon = epsilon_decay(total_frames)\n",
    "\n",
    "    # Reset Environment for each game\n",
    "    state = get_state(env.reset())\n",
    "    episode_score = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        total_frames += 1\n",
    "        action = q.sample_action(state.to(device), epsilon)\n",
    "\n",
    "        obs, reward, done, info = env.step(action)\n",
    "\n",
    "        next_state = get_state(obs)\n",
    "\n",
    "        done_mask = 0.0 if done else 1.0\n",
    "        memory.put((state,action,reward,next_state,done_mask))\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        score += reward\n",
    "        episode_score += reward\n",
    "\n",
    "        if memory.size() > 10000:\n",
    "            train(q, q_target, memory, optimizer, batch_size, gamma)\n",
    "        if total_frames%update_target_interval == 0:\n",
    "            q_target.load_state_dict(q.state_dict())\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    if episode_score > best_episode_score:\n",
    "        best_episode_score = episode_score\n",
    "\n",
    "    if episode%print_interval==0 and episode!=0:\n",
    "        print(\"n_episode : {}, Total Frames : {}, Average Score : {:.1f}, Episode Score : {:.1f}, Best Score : {:.1f}, n_buffer : {}, eps : {:.1f}%\".format(\n",
    "            episode, total_frames, score/episode, episode_score, best_episode_score, memory.size(), epsilon*100))\n",
    "\n",
    "''' # Microsoft Tensorwatch Watcher for Visualizing Training\n",
    "watcher.observe(\n",
    "    episode = episode,\n",
    "    episode_score = episode_score,\n",
    "    total_score = score,\n",
    "    buffer_size = memory.size(),\n",
    "    epsilon = epsilon,\n",
    "    frames = total_frames,\n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.2262e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#train(q, q_target, memory, optimizer, batch_size, gamma)\n",
    "s,a,r,s_prime,done_mask = memory.sample(batch_size)\n",
    "\n",
    "q_out = q(s)\n",
    "# collect output from the chosen action dimension\n",
    "q_a = q_out.gather(1,a) \n",
    "\n",
    "# most reward we get in next state s_prime\n",
    "max_q_prime = q_target(s_prime).max(1)[0].unsqueeze(1)\n",
    "target = r + gamma * max_q_prime * done_mask\n",
    "\n",
    "# how much is our policy different from the true target \n",
    "loss = F.smooth_l1_loss(q_a, target)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "\n",
    "#with amp.scale_loss(loss, optimizer) as scaled_loss: # playing around with mixed-precision training\n",
    "    #scaled_loss.backward()\n",
    "print(loss)\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double DQN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.8967e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#s,a,r,s_prime,done_mask = memory.sample(batch_size)\n",
    "# Q_out is the observed transitions given the current network\n",
    "q_out = q(s)\n",
    "# collect output from the chosen action dimension\n",
    "q_a = q_out.gather(1,a)\n",
    "\n",
    "# DDQN Update\n",
    "argmax_q = q(s_prime).argmax(1).unsqueeze(1)\n",
    "# most reward we get in next state s_prime\n",
    "q_prime = q_target(s_prime).gather(1,argmax_q)\n",
    "# most reward we get in next state s_prime\n",
    "target = r + gamma * q_prime * done_mask\n",
    "\n",
    "# how much is our policy different from the true target \n",
    "loss = F.smooth_l1_loss(q_a, target)\n",
    "optimizer.zero_grad()\n",
    "print(loss)\n",
    "#with amp.scale_loss(loss, optimizer) as scaled_loss: # playing around with mixed-precision training\n",
    "    #scaled_loss.backward()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1825, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "argmax_q = q(s_prime).argmax(1).unsqueeze(1)\n",
    "print(argmax_q.shape)\n",
    "q_prime = q_target(s_prime).gather(1,argmax_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6189],\n",
       "        [-0.6451],\n",
       "        [-0.5358],\n",
       "        [-0.5710],\n",
       "        [-0.6022],\n",
       "        [-0.5232],\n",
       "        [-0.5109],\n",
       "        [-0.5574],\n",
       "        [-0.4999],\n",
       "        [-0.4616],\n",
       "        [-0.5134],\n",
       "        [-0.5512],\n",
       "        [-0.4987],\n",
       "        [-0.4735],\n",
       "        [-0.5795],\n",
       "        [-0.4905],\n",
       "        [-0.4650],\n",
       "        [-0.4160],\n",
       "        [-0.5555],\n",
       "        [-0.6042],\n",
       "        [-0.5089],\n",
       "        [-0.4629],\n",
       "        [-0.5191],\n",
       "        [-0.4783],\n",
       "        [-0.4981],\n",
       "        [-0.8057],\n",
       "        [-0.4388],\n",
       "        [-0.5428],\n",
       "        [-0.5365],\n",
       "        [-0.5361],\n",
       "        [-0.3484],\n",
       "        [-0.7088]], device='cuda:0', grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_prime - max_q_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3769],\n",
       "        [0.4085],\n",
       "        [0.3843],\n",
       "        [0.3430],\n",
       "        [0.3344],\n",
       "        [0.2934],\n",
       "        [0.3056],\n",
       "        [0.2465],\n",
       "        [0.3038],\n",
       "        [0.3145],\n",
       "        [0.2961],\n",
       "        [0.3676],\n",
       "        [0.3487],\n",
       "        [0.2906],\n",
       "        [0.3746],\n",
       "        [0.2893],\n",
       "        [0.3307],\n",
       "        [0.2630],\n",
       "        [0.3658],\n",
       "        [0.3887],\n",
       "        [0.3147],\n",
       "        [0.2595],\n",
       "        [0.3265],\n",
       "        [0.2814],\n",
       "        [0.2880],\n",
       "        [0.4896],\n",
       "        [0.2535],\n",
       "        [0.3020],\n",
       "        [0.3423],\n",
       "        [0.3628],\n",
       "        [0.3557],\n",
       "        [0.4702]], device='cuda:0', grad_fn=<UnsqueezeBackward0>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_q_prime - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7731],\n",
       "        [0.7055],\n",
       "        [0.8860],\n",
       "        [0.8373],\n",
       "        [0.7286],\n",
       "        [0.7353],\n",
       "        [0.7801],\n",
       "        [0.6656],\n",
       "        [0.7737],\n",
       "        [0.8700],\n",
       "        [0.7707],\n",
       "        [0.7990],\n",
       "        [0.8587],\n",
       "        [0.7206],\n",
       "        [0.7947],\n",
       "        [0.8649],\n",
       "        [0.8186],\n",
       "        [0.8612],\n",
       "        [0.7996],\n",
       "        [0.7409],\n",
       "        [0.8448],\n",
       "        [0.7313],\n",
       "        [0.8361],\n",
       "        [0.8069],\n",
       "        [0.8215],\n",
       "        [0.7335],\n",
       "        [0.8288],\n",
       "        [0.7494],\n",
       "        [0.7890],\n",
       "        [0.7461],\n",
       "        [1.0747],\n",
       "        [0.8251]], device='cuda:0', grad_fn=<UnsqueezeBackward0>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "argmax_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record trained agent gameplay\n",
    "\n",
    "frames = []\n",
    "\n",
    "env.reset()\n",
    "current_s = get_screen()\n",
    "done = False\n",
    "last_s = get_screen()\n",
    "current_s = get_screen()\n",
    "s = last_s - current_s\n",
    "epsilon = 0.0\n",
    "while not done:\n",
    "    a = q.sample_action(s, epsilon) + 2\n",
    "    \n",
    "    # use environment's frame instead of preprocessed get_screen()\n",
    "    next_frame, _, done, info = env.step(a)\n",
    "    frames.append(next_frame)\n",
    "    last_s = current_s\n",
    "    current_s = get_screen()\n",
    "    s_prime = last_s - current_s\n",
    "\n",
    "    done_mask = 0.0 if done else 1.0\n",
    "    s = s_prime\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save game to video \n",
    "height, width = frames[0].shape[:2] \n",
    "\n",
    "writer = cv2.VideoWriter_fourcc('M','J','P','G')\n",
    "fps = 30\n",
    "video_file = 'playback.avi'\n",
    "out = cv2.VideoWriter(video_file, writer, fps, (width,height))\n",
    "for frame in frames:\n",
    "    out.write(frame)\n",
    "\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DDQN import DDQN\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wrappers import make_env\n",
    "env = gym.make('PongNoFrameskip-v4')\n",
    "env = make_env(env)\n",
    "ddqn = DDQN(env, save_location = \"checkpoints/pong/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:09<15:31,  9.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_episode : 1, Total Frames : 834, Average Score : -20.0, Episode Score : -20.0, Best Score : -20.0, n_buffer : 834, eps : 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▏         | 2/100 [00:18<15:20,  9.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_episode : 2, Total Frames : 1679, Average Score : -20.5, Episode Score : -21.0, Best Score : -20.0, n_buffer : 1679, eps : 99.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|▎         | 3/100 [00:27<14:45,  9.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_episode : 3, Total Frames : 2440, Average Score : -20.7, Episode Score : -21.0, Best Score : -20.0, n_buffer : 2440, eps : 98.4%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▍         | 4/100 [00:37<15:08,  9.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_episode : 4, Total Frames : 3372, Average Score : -20.8, Episode Score : -21.0, Best Score : -20.0, n_buffer : 3372, eps : 97.6%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▌         | 5/100 [00:49<15:57, 10.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_episode : 5, Total Frames : 4420, Average Score : -20.6, Episode Score : -20.0, Best Score : -20.0, n_buffer : 4420, eps : 96.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|▌         | 6/100 [00:58<15:27,  9.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_episode : 6, Total Frames : 5260, Average Score : -20.7, Episode Score : -21.0, Best Score : -20.0, n_buffer : 5260, eps : 95.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 7/100 [01:09<15:40, 10.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_episode : 7, Total Frames : 6230, Average Score : -20.6, Episode Score : -20.0, Best Score : -20.0, n_buffer : 6230, eps : 94.9%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|▊         | 8/100 [01:20<16:02, 10.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_episode : 8, Total Frames : 7254, Average Score : -20.4, Episode Score : -19.0, Best Score : -19.0, n_buffer : 7254, eps : 94.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|▉         | 9/100 [01:28<14:56,  9.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_episode : 9, Total Frames : 8013, Average Score : -20.4, Episode Score : -21.0, Best Score : -19.0, n_buffer : 8013, eps : 93.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 10/100 [01:37<14:07,  9.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_episode : 10, Total Frames : 8772, Average Score : -20.5, Episode Score : -21.0, Best Score : -19.0, n_buffer : 8772, eps : 92.4%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|█         | 11/100 [01:45<13:31,  9.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_episode : 11, Total Frames : 9529, Average Score : -20.5, Episode Score : -21.0, Best Score : -19.0, n_buffer : 9529, eps : 91.7%\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'F' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e7cf4566ff80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mddqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/RL/DQN.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, num_episodes)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtotal_frames\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_frame_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;31m# Copy policy weights to target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/RL/DDQN.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# how much is our policy different from the true target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmooth_l1_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'F' is not defined"
     ]
    }
   ],
   "source": [
    "ddqn.run(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
