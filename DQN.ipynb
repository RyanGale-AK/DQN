{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorwatch as tw\n",
    "\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import collections\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from wrappers import make_env\n",
    "\n",
    "NOTEBOOK_MODE = True\n",
    "if NOTEBOOK_MODE:\n",
    "    from tqdm.notebook import tqdm\n",
    "else:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "\n",
    "env = gym.make('Pong-v0').unwrapped\n",
    "if NOTEBOOK_MODE:\n",
    "    # set up matplotlib to open viewing window\n",
    "    is_ipython = 'inline' in matplotlib.get_backend()\n",
    "    if is_ipython:\n",
    "        from IPython import display\n",
    "\n",
    "    plt.ion()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import argparse\n",
    "import math\n",
    "import os\n",
    "import tensorwatch as tw\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# from apex import amp # playing around with mixed-precision training\n",
    "\n",
    "# Local Imports\n",
    "from models import Qnet\n",
    "from wrappers import make_env\n",
    "from memory import ReplayBuffer\n",
    "from helpers import saveTrainedGameplay, get_state\n",
    "from settings import device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(64, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "def get_screen():\n",
    "    # convert to channel,h,w dimensions\n",
    "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "    \n",
    "    # erase background\n",
    "    screen[screen == 72] = 0 \n",
    "    screen[screen == 74] = 0 \n",
    "    screen[screen == 144] = 0 \n",
    "    screen[screen != 0] = 213\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    \n",
    "    screen = torch.from_numpy(screen)\n",
    "    \n",
    "    # convert to batch,channel,h,w dimensions\n",
    "    return resize(screen).unsqueeze(0).to(device)\n",
    "\n",
    "if NOTEBOOK_MODE:\n",
    "    env.reset()\n",
    "    # run game for a bit to load the ball and opponent paddle\n",
    "    for i in range(50):\n",
    "        env.step(0)\n",
    "    plt.figure()\n",
    "    plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
    "               interpolation='none')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ReplayBuffer(size=100)\n",
    "\n",
    "#memory.put((state,action,reward,next_state,done_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#memory.put((state,1,0.0,next_state,0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Memory Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from memory import ReplayBuffer\n",
    "import gym\n",
    "from helpers import get_state\n",
    "from wrappers import make_env\n",
    "env = gym.make('PongNoFrameskip-v4')\n",
    "env = make_env(env)\n",
    "memory = ReplayBuffer(size=100000)\n",
    "state = get_state(env.reset())\n",
    "next_state = get_state(env.step(0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20000):\n",
    "    if i % 100 == 0:\n",
    "        env.reset()\n",
    "    next_state = get_state(env.step(1)[0])\n",
    "    memory.put((state,1,0.0,next_state,0.0))\n",
    "    state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "device = 'cuda'\n",
    "for i in range(10000):\n",
    "    s,a,r,s_prime,done_mask = memory.sample(32)\n",
    "    s = torch.Tensor(s).to(device)\n",
    "    a = torch.LongTensor(a).to(device)\n",
    "    r = torch.Tensor(r).to(device)\n",
    "    s_prime = torch.Tensor(s_prime).to(device)\n",
    "    done_mask = torch.Tensor(done_mask).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 52,  87,  87,  ..., 236, 236, 236],\n",
       "         [ 52,  87,  87,  ..., 236, 236, 236],\n",
       "         [ 52,  87,  87,  ..., 236, 236, 236],\n",
       "         ...,\n",
       "         [ 87,  87,  87,  ..., 236, 236, 236],\n",
       "         [ 87,  87,  87,  ..., 236, 236, 236],\n",
       "         [ 87,  87,  87,  ..., 236, 236, 236]],\n",
       "\n",
       "        [[ 52,  87,  87,  ..., 236, 236, 236],\n",
       "         [ 52,  87,  87,  ..., 236, 236, 236],\n",
       "         [ 52,  87,  87,  ..., 236, 236, 236],\n",
       "         ...,\n",
       "         [ 87,  87,  87,  ..., 236, 236, 236],\n",
       "         [ 87,  87,  87,  ..., 236, 236, 236],\n",
       "         [ 87,  87,  87,  ..., 236, 236, 236]],\n",
       "\n",
       "        [[ 52,  87,  87,  ..., 236, 236, 236],\n",
       "         [ 52,  87,  87,  ..., 236, 236, 236],\n",
       "         [ 52,  87,  87,  ..., 236, 236, 236],\n",
       "         ...,\n",
       "         [ 87,  87,  87,  ..., 236, 236, 236],\n",
       "         [ 87,  87,  87,  ..., 236, 236, 236],\n",
       "         [ 87,  87,  87,  ..., 236, 236, 236]],\n",
       "\n",
       "        [[ 52,  87,  87,  ..., 236, 236, 236],\n",
       "         [ 52,  87,  87,  ..., 236, 236, 236],\n",
       "         [ 52,  87,  87,  ..., 236, 236, 236],\n",
       "         ...,\n",
       "         [ 87,  87,  87,  ..., 236, 236, 236],\n",
       "         [ 87,  87,  87,  ..., 236, 236, 236],\n",
       "         [ 87,  87,  87,  ..., 236, 236, 236]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.as_tensor(memory.sample(10)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Prioritized Memory Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from memory import PrioritizedReplayBuffer\n",
    "import gym\n",
    "from helpers import get_state\n",
    "from wrappers import make_env\n",
    "env = gym.make('PongNoFrameskip-v4')\n",
    "env = make_env(env)\n",
    "memory = PrioritizedReplayBuffer(size=10000, alpha = 1)\n",
    "\n",
    "state = get_state(env.reset())\n",
    "next_state = get_state(env.step(0)[0])\n",
    "for _ in range(10000):\n",
    "    a = np.random.choice([0,1,2,3])\n",
    "    next_state = get_state(env.step(a)[0])\n",
    "    memory.put((state,a,0.0,next_state,1.0))\n",
    "    state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s,a,r,s_prime,done_mask,weights,idxes = memory.sample(4, 0.8)\n",
    "print(weights)\n",
    "print(idxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Qnet\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "h, w = 84, 84\n",
    "gamma = 0.98\n",
    "device='cpu'\n",
    "q = Qnet(h,w, in_channels = 4, n_actions = 4).to(device)\n",
    "q_target = Qnet(h,w, in_channels = 4, n_actions = 4).to(device)\n",
    "\n",
    "# Load policy weights into target network\n",
    "q_target.load_state_dict(q.state_dict())\n",
    "\n",
    "s = torch.Tensor(s).to(device)\n",
    "a = torch.LongTensor(a).to(device)\n",
    "r = torch.Tensor(r).to(device)\n",
    "s_prime = torch.Tensor(s_prime).to(device)\n",
    "done_mask = torch.Tensor(done_mask).to(device)\n",
    "\n",
    "q_out = q(s)\n",
    "# collect output from the chosen action dimension\n",
    "q_a = q_out.gather(1,a)\n",
    "\n",
    "# most reward we get in next state s_prime\n",
    "argmax_q = q(s_prime).argmax(1).unsqueeze(1)\n",
    "q_prime = q_target(s_prime).gather(1,argmax_q)\n",
    "target = r + gamma * q_prime\n",
    "\n",
    "TD_error = target-q_a\n",
    "print(target-q_a)\n",
    "loss = F.smooth_l1_loss(q_a, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.Tensor(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.Tensor([0.03,0.5,0.2,0.8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((TD_error**2).view(-1) * w).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.smooth_l1_loss(target.view(-1)*w, q_a.view(-1)*w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(TD_error.view(-1) * torch.Tensor(weights)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "torch.FloatTensor(np.expand_dims(np.uint8(x).transpose(), 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "torch.ByteTensor(x).permute((2,0,1)).unsqueeze(0).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pympler.asizeof import asizeof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.getsizeof(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asizeof(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asizeof(memory.buffer[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.buffer[0][0].type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = memory.buffer[0][0]\n",
    "a.element_size() * a.nelement() # 4 * 1 * 4 * 84 * 84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.element_size() * a.nelement() * 2 * 100000 / 1e+9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_cost = 4 * 1 * 4 * 84 * 84 * 2 * 100000 / 1e9\n",
    "print(\"%sGB\" % buffer_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Optimizes our training policy by computing the Huber Loss between our minibatch of samples and the maximum possible reward for the next state(s)\n",
    "    Huber Loss here is defined as:\n",
    "    loss(x,y) = \\frac{1}{n}\\sum{z_i}, where z_i = 0.5(x_i-y_i)^2; if |x_i - y_i| < 1 or \n",
    "                                                = |x_i - y_i| - 0.5; otherwise\n",
    "'''\n",
    "def train(q, q_target, memory, optimizer, batch_size, gamma):\n",
    "    s,a,r,s_prime,done_mask = memory.sample(batch_size)\n",
    "    \n",
    "    q_out = q(s)\n",
    "    # collect output from the chosen action dimension\n",
    "    q_a = q_out.gather(1,a) \n",
    "    \n",
    "    # most reward we get in next state s_prime\n",
    "    max_q_prime = q_target(s_prime).max(1)[0].unsqueeze(1)\n",
    "    target = r + gamma * max_q_prime * done_mask\n",
    "\n",
    "    # how much is our policy different from the true target \n",
    "    loss = F.smooth_l1_loss(q_a, target)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    #with amp.scale_loss(loss, optimizer) as scaled_loss: # playing around with mixed-precision training\n",
    "    \t#scaled_loss.backward()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "#def main(num_episodes, episode_start = 1, saved_model = None, save_loc = 'checkpoints/tmp/'):\n",
    "#watcher = tw.Watcher()\n",
    "num_episodes = 100\n",
    "episode_start = 1\n",
    "saved_model = 'checkpoints/4channel/target_bot_1500.pt'\n",
    "save_loc = None\n",
    "# Model parameters \n",
    "learning_rate = 1e-4 # 0.0001 matches paper\n",
    "gamma = 0.98\n",
    "buffer_limit = 10 ** 5 # paper uses 1M last frames, but this is expensive, so we try 10x less\n",
    "batch_size = 32\n",
    "\n",
    "# Epsilon Decay Parameters\n",
    "eps_start = 1\n",
    "eps_end = 0.01\n",
    "decay_factor = 10 ** 5\n",
    "\n",
    "epsilon_decay = lambda x: eps_end + (eps_start - eps_end) * \\\n",
    "    math.exp(-1. * x / decay_factor)\n",
    "\n",
    "env = gym.make('PongNoFrameskip-v4')\n",
    "env = make_env(env)\n",
    "h, w = 84, 84\n",
    "\n",
    "\n",
    "# Initialize the policy (q) network, target network, and experience replay buffer\n",
    "q = Qnet(h,w, in_channels = 4, n_actions = 4).to(device)\n",
    "q_target = Qnet(h,w, in_channels = 4, n_actions = 4).to(device)\n",
    "memory = ReplayBuffer(buffer_limit)\n",
    "\n",
    "\n",
    "# Load policy weights into target network\n",
    "q_target.load_state_dict(q.state_dict())\n",
    "optimizer = optim.Adam(q.parameters(), lr=learning_rate)\n",
    "\n",
    "save_interval = 250\n",
    "print_interval = 1\n",
    "update_target_interval = 1000 # every 1000 frames\n",
    "score = 0.0\n",
    "\n",
    "\n",
    "#[q, q_target], optimizer = amp.initialize([q, q_target], optimizer, opt_level=\"O1\") #playing around with mixed-precision training\n",
    "total_frames = 0\n",
    "best_episode_score = -100\n",
    "for episode in tqdm(range(episode_start,episode_start + num_episodes)):\n",
    "    # anneal 100% to 1% over training\n",
    "    epsilon = epsilon_decay(total_frames)\n",
    "\n",
    "    # Reset Environment for each game\n",
    "    state = get_state(env.reset())\n",
    "    episode_score = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        total_frames += 1\n",
    "        action = q.sample_action(state.to(device), epsilon)\n",
    "\n",
    "        obs, reward, done, info = env.step(action)\n",
    "\n",
    "        next_state = get_state(obs)\n",
    "\n",
    "        done_mask = 0.0 if done else 1.0\n",
    "        memory.put((state,action,reward,next_state,done_mask))\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        score += reward\n",
    "        episode_score += reward\n",
    "\n",
    "        if memory.size() > 10000:\n",
    "            train(q, q_target, memory, optimizer, batch_size, gamma)\n",
    "        if total_frames%update_target_interval == 0:\n",
    "            q_target.load_state_dict(q.state_dict())\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    if episode_score > best_episode_score:\n",
    "        best_episode_score = episode_score\n",
    "\n",
    "    if episode%print_interval==0 and episode!=0:\n",
    "        print(\"n_episode : {}, Total Frames : {}, Average Score : {:.1f}, Episode Score : {:.1f}, Best Score : {:.1f}, n_buffer : {}, eps : {:.1f}%\".format(\n",
    "            episode, total_frames, score/episode, episode_score, best_episode_score, memory.size(), epsilon*100))\n",
    "\n",
    "''' # Microsoft Tensorwatch Watcher for Visualizing Training\n",
    "watcher.observe(\n",
    "    episode = episode,\n",
    "    episode_score = episode_score,\n",
    "    total_score = score,\n",
    "    buffer_size = memory.size(),\n",
    "    epsilon = epsilon,\n",
    "    frames = total_frames,\n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train(q, q_target, memory, optimizer, batch_size, gamma)\n",
    "s,a,r,s_prime,done_mask = memory.sample(batch_size)\n",
    "\n",
    "q_out = q(s)\n",
    "# collect output from the chosen action dimension\n",
    "q_a = q_out.gather(1,a) \n",
    "\n",
    "# most reward we get in next state s_prime\n",
    "max_q_prime = q_target(s_prime).max(1)[0].unsqueeze(1)\n",
    "target = r + gamma * max_q_prime * done_mask\n",
    "\n",
    "# how much is our policy different from the true target \n",
    "loss = F.smooth_l1_loss(q_a, target)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "\n",
    "#with amp.scale_loss(loss, optimizer) as scaled_loss: # playing around with mixed-precision training\n",
    "    #scaled_loss.backward()\n",
    "print(loss)\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DQN import DQN\n",
    "import gym\n",
    "from wrappers import make_env\n",
    "env = gym.make('PongNoFrameskip-v4')\n",
    "env = make_env(env)\n",
    "dqn = DQN(env, save_location = \"checkpoints/pong/\")\n",
    "dqn.run(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double DQN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s,a,r,s_prime,done_mask = memory.sample(batch_size)\n",
    "# Q_out is the observed transitions given the current network\n",
    "q_out = q(s)\n",
    "# collect output from the chosen action dimension\n",
    "q_a = q_out.gather(1,a)\n",
    "\n",
    "# DDQN Update\n",
    "argmax_q = q(s_prime).argmax(1).unsqueeze(1)\n",
    "# most reward we get in next state s_prime\n",
    "q_prime = q_target(s_prime).gather(1,argmax_q)\n",
    "# most reward we get in next state s_prime\n",
    "target = r + gamma * q_prime * done_mask\n",
    "\n",
    "# how much is our policy different from the true target \n",
    "loss = F.smooth_l1_loss(q_a, target)\n",
    "optimizer.zero_grad()\n",
    "print(loss)\n",
    "#with amp.scale_loss(loss, optimizer) as scaled_loss: # playing around with mixed-precision training\n",
    "    #scaled_loss.backward()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "argmax_q = q(s_prime).argmax(1).unsqueeze(1)\n",
    "print(argmax_q.shape)\n",
    "q_prime = q_target(s_prime).gather(1,argmax_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record trained agent gameplay\n",
    "\n",
    "frames = []\n",
    "\n",
    "env.reset()\n",
    "current_s = get_screen()\n",
    "done = False\n",
    "last_s = get_screen()\n",
    "current_s = get_screen()\n",
    "s = last_s - current_s\n",
    "epsilon = 0.0\n",
    "while not done:\n",
    "    a = q.sample_action(s, epsilon) + 2\n",
    "    \n",
    "    # use environment's frame instead of preprocessed get_screen()\n",
    "    next_frame, _, done, info = env.step(a)\n",
    "    frames.append(next_frame)\n",
    "    last_s = current_s\n",
    "    current_s = get_screen()\n",
    "    s_prime = last_s - current_s\n",
    "\n",
    "    done_mask = 0.0 if done else 1.0\n",
    "    s = s_prime\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save game to video \n",
    "height, width = frames[0].shape[:2] \n",
    "\n",
    "writer = cv2.VideoWriter_fourcc('M','J','P','G')\n",
    "fps = 30\n",
    "video_file = 'playback.avi'\n",
    "out = cv2.VideoWriter(video_file, writer, fps, (width,height))\n",
    "for frame in frames:\n",
    "    out.write(frame)\n",
    "\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from DDQN import DDQN\n",
    "import gym\n",
    "from wrappers import make_env\n",
    "env = gym.make('PongNoFrameskip-v4')\n",
    "env = make_env(env)\n",
    "ddqn = DDQN(env, save_location = \"checkpoints/pong/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|          | 1/100 [00:01<02:01,  1.23s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_episode : 1, Total Frames : 957, Average Score : -20.0, Episode Score : -20.0, Best Score : -20.0, n_buffer : 957, eps : 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|▏         | 2/100 [00:25<13:23,  8.20s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_episode : 2, Total Frames : 1892, Average Score : -19.5, Episode Score : -19.0, Best Score : -19.0, n_buffer : 1892, eps : 99.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  3%|▎         | 3/100 [00:55<23:35, 14.60s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_episode : 3, Total Frames : 2957, Average Score : -19.7, Episode Score : -20.0, Best Score : -19.0, n_buffer : 2957, eps : 98.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  4%|▍         | 4/100 [01:25<30:53, 19.30s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_episode : 4, Total Frames : 4031, Average Score : -19.5, Episode Score : -19.0, Best Score : -19.0, n_buffer : 4031, eps : 97.1%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-e7cf4566ff80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mddqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/RL/DQN.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, num_episodes)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtotal_frames\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_frame_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \u001b[0;31m# Copy policy weights to target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/RL/DDQN.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprioritized_replay\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms_prime\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdone_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/RL/memory.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, batch_size, beta)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0midxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sample_proportional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/RL/memory.py\u001b[0m in \u001b[0;36m_sample_proportional\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mmass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mevery_range_len\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mevery_range_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_it_sum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_prefixsum_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/RL/segment_tree.py\u001b[0m in \u001b[0;36mfind_prefixsum_idx\u001b[0;34m(self, prefixsum)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_capacity\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# while non-leaf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mprefixsum\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m                 \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gym/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ddqn.run(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_decay = lambda x: eps_end + (eps_start - eps_end) * math.exp(-1. * x / decay_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999727600421425"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "eps_end = 1\n",
    "eps_start = 0.4\n",
    "decay_factor = 100000\n",
    "epsilon_decay(1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
