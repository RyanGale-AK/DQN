{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import collections\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "NOTEBOOK_MODE = False\n",
    "if NOTEBOOK_MODE:\n",
    "    from tqdm.notebook import tqdm\n",
    "else:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "\n",
    "env = gym.make('Pong-v0').unwrapped\n",
    "if NOTEBOOK_MODE:\n",
    "    # set up matplotlib to open viewing window\n",
    "    is_ipython = 'inline' in matplotlib.get_backend()\n",
    "    if is_ipython:\n",
    "        from IPython import display\n",
    "\n",
    "    plt.ion()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMkAAAD6CAYAAAALKGMGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAANr0lEQVR4nO3dX4xc9XnG8e/j9S4YYtdsbIhlQ6GNU0BpMdKKWqIXLYTKJajmgiCjprGEJV+QCqJGSqCViiL1IpQoVJXKhVVbcaSUPyWJjFDU1nJBUaVi2MQkNTHBDkod/xGOsV07MmDv+u3FHDd7Zmf3Pd6dOXPWfj7SauY9f/a8mvXj3zlz5sxRRGBmU5vX7wbMms4hMUs4JGYJh8Qs4ZCYJRwSs8SsQiJpjaSfSton6dFuNWXWJJrpeRJJA8DbwF3AAeB14IGI+MnU6ywKWDpxStsSl3Vaa0b9TWfevPFSfd11+0v1/Pnl+WNjA6V6//7rJv3Oc+cGJk27VMxr+6/2uraXZ/78cj02Vq73l19+AM6dm31fF+YgEcc7/mOb32liRbcB+yLiHQBJzwJrgSlD0grI302o2/9h/XaFFi801JMHyyuv/N9S/fjjD5fqJUtOlOqjRxeX6ocf/odJv/PUqd9om1L7X7lvrryyXD/+eLlesqRcHz1arh8uv/wAnDo1+74uzP1TzpnN7tZy4BcT6gPFtBJJGyWNShqFk7PYnFl/zCYknYamSf/NR8SmiBiJiBFYNIvNmfXHbHa3DgDXTqhXAIemX2UB8LvTzB/rMG22ny2bnOWhoTOlevnyg6V6y5YNpfrBBzdPu/5U27lUDA2V6+Vt+xNbtpTrBx+cfv2mmc1I8jqwUtINkoaAdcCL3WnLrDlmPJJExJikvwD+jdYR+JaIeLNrnZk1xGx2t4iI7wHf61IvZo00q5DMTKfjjv46ffqKUv3qq79fqtete6bOdua806fL9auvlut16+rrpRv8sRSzhENilnBIzBIOiVmiDwfuzSOVT1gODp6ddr5NT23nVQcHp5/fdB5JzBIOiVnCITFLXJLHJGfOlP9vWLjweKl+8skvTTu/ff1L3Zm2z3suXFiun3xy+vnt6zeN/9pmCYfELOGQmCVm/EUQMzE4+MlYsuT52rbXMvlN+YGB8ocsb7zxrVK9YMEHpfr99y8v1W+9deOk3zk+Pttr8eeugbavKrix7eVZsKBcv/9+uX6r/PIDMD4+eVovHT16P2fP7u54BscjiVnCITFLOCRmiVrPk6xcCd/8Zp1bhM7HBuWd6LNnV5XXiPKuafbZrqm3c2k62/bytB/2Zp/t6ofPfW7qeR5JzBIOiVnCITFL1HpMMn8+LF2aL1e/TscYdilp/1LviTySmCUcErOEQ2KWcEjMErUeuB88CI89VucWzao5eHDqeR5JzBIOiVnCITFL1HpMcuwYPOMvaLc5xiOJWcIhMUukIZG0RdIRSbsnTBuWtF3S3uLxqt62adY/VUaSbwBr2qY9CuyIiJXAjqI2uyilIYmI7wPH2iavBbYWz7cC93a5L7PGmOkxyTURcRigeLx6qgUlbZQ0Kml0ctbMmq/nB+4RsSkiRiJiBIZ7vTmzrptpSN6VtAygeDzSvZbMmmWmIXkRWF88Xw9s6047Zs1T5S3gZ4D/An5H0gFJG4CvAndJ2gvcVdRmF6X0YykR8cAUs+7sci9mjeQz7mYJh8Qs4ZCYJRwSs4RDYpZwSMwSDolZwiExSzgkZgmHxCzhkJglHBKzhENilnBIzBIOiVnCITFLOCRmCYfELOGQmCUcErOEQ2KWcEjMEg6JWcIhMUs4JGYJh8Qs4ZCYJRwSs4RDYpZwSMwSDolZwiExSzgkZokqt4O7VtLLkvZIelPSI8X0YUnbJe0tHq/qfbtm9asykowBX4yIm4DVwOcl3Qw8CuyIiJXAjqI2u+ikIYmIwxHxw+L5KWAPsBxYC2wtFtsK3NurJs366YKOSSRdD9wK7ASuiYjD0AoScPUU62yUNCppFI7NrluzPqgcEkkfAb4NfCEiTlZdLyI2RcRIRIzA8Ex6NOurSiGRNEgrIN+KiO8Uk9+VtKyYvww40psWzfqryrtbAjYDeyLi6xNmvQisL56vB7Z1vz2z/ptfYZnbgT8H/lvSG8W0vwK+CjwvaQOwH/hMb1o06680JBHxn4CmmH1nd9sxax6fcTdLOCRmCYfELOGQmCWqvLtl1neDg+X6E58o12+/Xa7Pnu3etj2SmCUcErOEQ2KWcEjMEj5wtzlh0aJy/bWvlevPfrZcv/de97btkcQs4ZCYJRwSs4SPSWxOGh+vb1seScwSDolZwiExS8zBY5KBtrrGndPCvA7/tdxzT7l+6aVyfe5c7/q5FKjt2tihoennd5NHErOEQ2KWcEjMEo0/Jpk3r7wzv2jRiVJ98mT5Qz3nzvU+9xGTp+3alS9jM3fmTLneuXP6+d3kkcQs4ZCYJRwSs4Sixp1n6ZMBz0+zRPs5EBge/mWpfvrph0r1Qw89XaqPHVva9hvqP49ic9H9ROzueLbFI4lZwiExSzgkZgmHxCzR+JOJUvmNhUWLTk4736zbPJKYJRwSs0SVeyZeLuk1ST+S9KakrxTTb5C0U9JeSc9JGsp+l9lcVGUk+RC4IyJuAVYBayStBp4AnoqIlcBxYEPv2vw1KUo/Zr2WhiRaflWUg8VPAHcALxTTtwL39qRDsz6reh/3geLOu0eA7cDPgBMRMVYscgBYPsW6GyWNShqFY93o2axWlUISEeMRsQpYAdwG3NRpsSnW3RQRIxExAsMz79SsTy7oPElEnJD0CrAaWCxpfjGarAAO9aC/SRdRvfvuNdPON+u2Ku9uLZW0uHi+APgUsAd4GbivWGw9sK1XTZr1U5WRZBmwVdIArVA9HxEvSfoJ8KykvwV2AZt72KdZ36QhiYgfA7d2mP4OreMTs4tawy666rROub/LLvuwVH/44WWlOqKH31JmFzFfdGU2Yw6JWcIhMUs0/nqS9mOMDz64om0JfxO19ZZHErOEQ2KWcEjMEg6JWaLxB+6T+UDd6uWRxCzhkJglHBKzhENilnBIzBIOiVnCITFLOCRmCYfELOGQmCUcErOEQ2KWcEjMEg6JWcIhMUs4JGYJh8Qs4ZCYJRwSs4RDYpZwSMwSDolZonJIijvw7pL0UlHfIGmnpL2SnpM01Ls2zfrnQkaSR2jdK/G8J4CnImIlcBzY0M3GzJqi6n3cVwCfBv6pqAXcAbxQLLIVuLcXDZr1W9WR5O+BL/Hrr0/8KHCiuD01wAFgeZd7M2uEKreovgc4EhE/mDi5w6Idb74oaaOkUUmjcGyGbZr1T5XvAr4d+FNJdwOXA4tojSyLJc0vRpMVwKFOK0fEJmATnL+xqNncko4kEfFYRKyIiOuBdcB/RMSfAS8D9xWLrQe29axLsz6azXmSLwN/KWkfrWOUzd1pyaxZLujWCxHxCvBK8fwd4Lbut2TWLD7jbpZwSMwSDolZwiExSzgkZgmHxCzhkJglHBKzhENilnBIzBIOiVnCITFLOCRmCYfELOGQmCUcErOEQ2KWcEjMEg6JWcIhMUs4JGYJh8QscUFfKTRb8+bBFVfUuUWzak6fnnqeRxKzhENilnBIzBK1HpN8/OOwaVOdWzSrZuPGqed5JDFLOCRmCYfELFHrMcngIHzsY3Vu0ayawcGp53kkMUs4JGaJSrtbkn4OnALGgbGIGJE0DDwHXA/8HLg/Io73pk2z/rmQkeSPImJVRIwU9aPAjohYCewoarOLzmwO3NcCf1g830rrXopfzlYaH5/FFs36oOpIEsC/S/qBpPPnJq+JiMMAxePVvWjQrN+qjiS3R8QhSVcD2yW9VXUDRag2AixbtmwGLZr1V6WRJCIOFY9HgO/SujX1u5KWARSPR6ZYd1NEjETEyPDwcHe6NqtRGhJJV0paeP458MfAbuBFYH2x2HpgW6+aNOunKrtb1wDflXR++X+OiH+V9DrwvKQNwH7gM71r06x/0pBExDvALR2mvwfc2YumzJrEZ9zNEoqI+jYm/RL4H2AJcLS2Dc+c++yepvf4mxGxtNOMWkPy/xuVRiecuW8s99k9c6HHqXh3yyzhkJgl+hWSufJ1EO6ze+ZCjx315ZjEbC7x7pZZwiExS9QaEklrJP1U0j5JjbpIS9IWSUck7Z4wbVjSdkl7i8er+tzjtZJelrRH0puSHmlon5dLek3Sj4o+v1JMv0HSzqLP5yQN9bPPqmoLiaQB4B+BPwFuBh6QdHNd26/gG8CatmlNu/pyDPhiRNwErAY+X7yGTevzQ+COiLgFWAWskbQaeAJ4qujzOLChjz1WVudIchuwLyLeiYgzwLO0rm5shIj4PnCsbfJaWlddUjzeW2tTbSLicET8sHh+CtgDLKd5fUZE/KooB4ufAO4AXiim973PquoMyXLgFxPqA8W0Jmvs1ZeSrgduBXbSwD4lDUh6g9Z1RtuBnwEnImKsWGQu/P2BekOiDtP8/vMMSPoI8G3gCxFxst/9dBIR4xGxClhBay/ipk6L1dvVzNQZkgPAtRPqFcChGrc/E5WuvqyTpEFaAflWRHynmNy4Ps+LiBO0viRkNbBY0vnLM+bC3x+oNySvAyuLdziGgHW0rm5sskZdfanWlW+bgT0R8fUJs5rW51JJi4vnC4BP0Tp+ehm4r1is731WFhG1/QB3A2/T2j/96zq3XaG3Z4DDwFlao94G4KO03i3aWzwO97nHP6C1i/Jj4I3i5+4G9vl7wK6iz93A3xTTfwt4DdgH/AtwWb//7lV+/LEUs4TPuJslHBKzhENilnBIzBIOiVnCITFLOCRmif8DHORX9j6aVBEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "def get_screen():\n",
    "    # convert to channel,h,w dimensions\n",
    "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "    \n",
    "    # erase background\n",
    "    screen[screen == 72] = 0 \n",
    "    screen[screen == 74] = 0 \n",
    "    screen[screen == 144] = 0 \n",
    "    screen[screen != 0] = 213\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    \n",
    "    screen = torch.from_numpy(screen)\n",
    "    \n",
    "    # convert to batch,channel,h,w dimensions\n",
    "    return resize(screen).unsqueeze(0).to(device)\n",
    "\n",
    "if NOTEBOOK_MODE:\n",
    "    env.reset()\n",
    "    # run game for a bit to load the ball and opponent paddle\n",
    "    for i in range(50):\n",
    "        env.step(0)\n",
    "    plt.figure()\n",
    "    plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
    "               interpolation='none')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0005\n",
    "gamma = 0.98\n",
    "buffer_limit = 50000\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "store previous sequence-action pairs to decorrelate temporal locality\n",
    "transitions are composed of state, action, reward, next_state, and done_mask\n",
    "\"\"\"\n",
    "x_tmp = []\n",
    "class ReplayBuffer():\n",
    "    def __init__(self):\n",
    "        self.buffer = collections.deque(maxlen=buffer_limit)\n",
    "    \n",
    "    def put(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "        \n",
    "    def sample(self, n):\n",
    "        mini_batch = random.sample(self.buffer, n)\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
    "        \n",
    "        for transition in mini_batch:\n",
    "            s, a, r, s_prime, done_mask = transition\n",
    "            s_lst.append(s)\n",
    "            a_lst.append([a])\n",
    "            r_lst.append([r])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            done_mask_lst.append([done_mask])\n",
    "        \n",
    "        #print(s_lst)\n",
    "        #print(len(s_lst))\n",
    "        #print(type(a_lst))\n",
    "        #print(type(r_lst))\n",
    "        #print(type(s_prime_lst))\n",
    "        #print(type(done_mask_lst))\n",
    "        #x_tmp.append(s_lst)\n",
    "            \n",
    "        return  torch.cat(s_lst).to(device), \\\n",
    "                torch.tensor(a_lst).to(device), \\\n",
    "                torch.tensor(r_lst).to(device),\\\n",
    "                torch.cat(s_prime_lst).to(device), \\\n",
    "                torch.tensor(done_mask_lst).to(device)\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, h, w = get_screen().shape\n",
    "\n",
    "class Qnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Qnet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        self.head = nn.Linear(linear_input_size, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))\n",
    "    \n",
    "    # action is either random or max probability estimated by Qnet\n",
    "    def sample_action(self, obs, epsilon):\n",
    "        out = self.forward(obs) # don't need if random action \n",
    "        coin = random.random()\n",
    "        if coin < epsilon:\n",
    "            return random.randint(0,1)\n",
    "        else:\n",
    "            return out.argmax().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(q, q_target, memory, optimizer):\n",
    "    for i in range(10):\n",
    "        s,a,r,s_prime,done_mask = memory.sample(batch_size)\n",
    "        \n",
    "        q_out = q(s)\n",
    "        # collect output from the chosen action dimension\n",
    "        q_a = q_out.gather(1,a) \n",
    "        \n",
    "        # most reward we get in next state s_prime\n",
    "        max_q_prime = q_target(s_prime).max(1)[0].unsqueeze(1)\n",
    "        target = r + gamma * max_q_prime * done_mask\n",
    "        # how much is our policy different from the true target \n",
    "        loss = F.smooth_l1_loss(q_a, target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(num_episodes):\n",
    "    env = gym.make('Pong-v0')\n",
    "    q = Qnet().to(device)\n",
    "    q_target = Qnet().to(device)\n",
    "    q_target.load_state_dict(q.state_dict())\n",
    "    memory = ReplayBuffer()\n",
    "    \n",
    "    save_interval = 250\n",
    "    print_interval = 50\n",
    "    score = 0.0\n",
    "    optimizer = optim.Adam(q.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for episode in tqdm(range(1,num_episodes+1)):\n",
    "        # anneal 8% to 1% over training\n",
    "        epsilon = max(0.01, 0.08 - 0.01*(episode/200))\n",
    "        env.reset()\n",
    "        current_s = get_screen()\n",
    "        done = False\n",
    "        last_s = get_screen()\n",
    "        current_s = get_screen()\n",
    "        s = last_s - current_s\n",
    "        episode_score = 0\n",
    "        while not done:\n",
    "            a = q.sample_action(s, epsilon)\n",
    "            # first variable would be s_prime but we have get_screen\n",
    "            _, r, done, info = env.step(a + 2)\n",
    "            last_s = current_s\n",
    "            current_s = get_screen()\n",
    "            s_prime = last_s - current_s\n",
    "            \n",
    "            done_mask = 0.0 if done else 1.0\n",
    "            memory.put((s,a,r/100.,s_prime,done_mask))\n",
    "            s = s_prime\n",
    "            \n",
    "            score += r\n",
    "            episode_score += r\n",
    "            if done:\n",
    "                break\n",
    "        if memory.size()>2000:\n",
    "            train(q, q_target, memory, optimizer)\n",
    "        \n",
    "        if episode%print_interval==0 and episode!=0:\n",
    "            q_target.load_state_dict(q.state_dict())\n",
    "            print(\"n_episode : {}, Average Score : {:.1f}, Episode Score : {:.1f}, n_buffer : {}, eps : {:.1f}%\".format(\n",
    "                episode, score/episode, episode_score, memory.size(), epsilon*100))\n",
    "            \n",
    "        if episode%save_interval==0 and episode!=0:\n",
    "            # save model weights \n",
    "            torch.save(q_target.state_dict(), 'checkpoints/target_bot_%s.pt' % episode)\n",
    "            torch.save(q.state_dict(), 'checkpoints/policy_bot_%s.pt' % episode)\n",
    "    # save final model weights \n",
    "    torch.save(q_target.state_dict(), 'checkpoints/target_bot_final.pt')\n",
    "    torch.save(q.state_dict(), 'checkpoints/policy_bot_final.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cf0239ffe2543169e3754bad1885a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a39a1fc53663>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-ef3af7c17423>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(num_episodes)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mlast_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_s\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mcurrent_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_screen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0ms_prime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlast_s\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcurrent_s\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-c398381b20f0>\u001b[0m in \u001b[0;36mget_screen\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# erase background\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mscreen\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mscreen\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m72\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mscreen\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mscreen\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m74\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mscreen\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mscreen\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m144\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = Qnet().to(device)\n",
    "q.load_state_dict(torch.load('checkpoints/target_bot_final.pt'))\n",
    "q.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record trained agent gameplay\n",
    "\n",
    "frames = []\n",
    "\n",
    "env.reset()\n",
    "current_s = get_screen()\n",
    "done = False\n",
    "last_s = get_screen()\n",
    "current_s = get_screen()\n",
    "s = last_s - current_s\n",
    "epsilon = 0.0\n",
    "while not done:\n",
    "    a = q.sample_action(s, epsilon) + 2\n",
    "    \n",
    "    # use environment's frame instead of preprocessed get_screen()\n",
    "    next_frame, _, done, info = env.step(a)\n",
    "    frames.append(next_frame)\n",
    "    last_s = current_s\n",
    "    current_s = get_screen()\n",
    "    s_prime = last_s - current_s\n",
    "\n",
    "    done_mask = 0.0 if done else 1.0\n",
    "    s = s_prime\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save game to video \n",
    "height, width = frames[0].shape[:2] \n",
    "\n",
    "writer = cv2.VideoWriter_fourcc('M','J','P','G')\n",
    "fps = 30\n",
    "video_file = 'playback.avi'\n",
    "out = cv2.VideoWriter(video_file, writer, fps, (width,height))\n",
    "for frame in frames:\n",
    "    out.write(frame)\n",
    "\n",
    "out.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
