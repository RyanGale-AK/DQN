{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import collections\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "NOTEBOOK_MODE = True\n",
    "if NOTEBOOK_MODE:\n",
    "    from tqdm.notebook import tqdm\n",
    "else:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "\n",
    "env = gym.make('Pong-v0').unwrapped\n",
    "if NOTEBOOK_MODE:\n",
    "    # set up matplotlib to open viewing window\n",
    "    is_ipython = 'inline' in matplotlib.get_backend()\n",
    "    if is_ipython:\n",
    "        from IPython import display\n",
    "\n",
    "    plt.ion()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(64, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "def get_screen():\n",
    "    # convert to channel,h,w dimensions\n",
    "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "    \n",
    "    # erase background\n",
    "    screen[screen == 72] = 0 \n",
    "    screen[screen == 74] = 0 \n",
    "    screen[screen == 144] = 0 \n",
    "    screen[screen != 0] = 213\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    \n",
    "    screen = torch.from_numpy(screen)\n",
    "    \n",
    "    # convert to batch,channel,h,w dimensions\n",
    "    return resize(screen).unsqueeze(0).to(device)\n",
    "\n",
    "if NOTEBOOK_MODE:\n",
    "    env.reset()\n",
    "    # run game for a bit to load the ball and opponent paddle\n",
    "    for i in range(50):\n",
    "        env.step(0)\n",
    "    plt.figure()\n",
    "    plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
    "               interpolation='none')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0005\n",
    "gamma = 0.98\n",
    "buffer_limit = 50000\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 84, 64])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_screen().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "store previous sequence-action pairs to decorrelate temporal locality\n",
    "transitions are composed of state, action, reward, next_state, and done_mask\n",
    "\"\"\"\n",
    "x_tmp = []\n",
    "class ReplayBuffer():\n",
    "    def __init__(self):\n",
    "        self.buffer = collections.deque(maxlen=buffer_limit)\n",
    "    \n",
    "    def put(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "        \n",
    "    def sample(self, n):\n",
    "        mini_batch = random.sample(self.buffer, n)\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
    "        \n",
    "        for transition in mini_batch:\n",
    "            s, a, r, s_prime, done_mask = transition\n",
    "            s_lst.append(s)\n",
    "            a_lst.append([a])\n",
    "            r_lst.append([r])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            done_mask_lst.append([done_mask])\n",
    "            \n",
    "        return  torch.cat(s_lst).to(device), \\\n",
    "                torch.tensor(a_lst).to(device), \\\n",
    "                torch.tensor(r_lst).to(device),\\\n",
    "                torch.cat(s_prime_lst).to(device), \\\n",
    "                torch.tensor(done_mask_lst).to(device)\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, h, w = get_screen().shape\n",
    "\n",
    "class Qnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Qnet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=8, stride=4)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        def conv2d_size_out(size, kernel_size = 4, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w, 8, 4), 4, 2), 3, 1)\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h, 8, 4), 4, 2), 3, 1)\n",
    "        linear_input_size = convw * convh * 64\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(linear_input_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))\n",
    "    \n",
    "    # action is either random or max probability estimated by Qnet\n",
    "    def sample_action(self, obs, epsilon):\n",
    "        out = self.forward(obs) # don't need if random action \n",
    "        coin = random.random()\n",
    "        if coin < epsilon:\n",
    "            return random.randint(0,1)\n",
    "        else:\n",
    "            return out.argmax().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(q, q_target, memory, optimizer):\n",
    "    s,a,r,s_prime,done_mask = memory.sample(batch_size)\n",
    "\n",
    "    q_out = q(s)\n",
    "    # collect output from the chosen action dimension\n",
    "    q_a = q_out.gather(1,a) \n",
    "\n",
    "    # most reward we get in next state s_prime\n",
    "    max_q_prime = q_target(s_prime).max(1)[0].unsqueeze(1)\n",
    "    target = r + gamma * max_q_prime * done_mask\n",
    "    # how much is our policy different from the true target \n",
    "    loss = F.smooth_l1_loss(q_a, target)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(num_episodes):\n",
    "    env = gym.make('Pong-v0')\n",
    "    q = Qnet().to(device)\n",
    "    q_target = Qnet().to(device)\n",
    "    q_target.load_state_dict(q.state_dict())\n",
    "    memory = ReplayBuffer()\n",
    "    \n",
    "    save_interval = 250\n",
    "    print_interval = 1\n",
    "    score = 0.0\n",
    "    optimizer = optim.Adam(q.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for episode in tqdm(range(1,num_episodes+1)):\n",
    "        # anneal 8% to 1% over training\n",
    "        epsilon = max(0.01, 0.08 - 0.01*(episode/200))\n",
    "        env.reset()\n",
    "        current_s = get_screen()\n",
    "        done = False\n",
    "        last_s = get_screen()\n",
    "        current_s = get_screen()\n",
    "        s = last_s - current_s\n",
    "        episode_score = 0\n",
    "        while not done:\n",
    "            a = q.sample_action(s, epsilon)\n",
    "            # first variable would be s_prime but we have get_screen\n",
    "            _, r, done, info = env.step(a + 2)\n",
    "            last_s = current_s\n",
    "            current_s = get_screen()\n",
    "            s_prime = last_s - current_s\n",
    "            \n",
    "            done_mask = 0.0 if done else 1.0\n",
    "            memory.put((s,a,r/100.,s_prime,done_mask))\n",
    "            s = s_prime\n",
    "            \n",
    "            score += r\n",
    "            episode_score += r\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            if memory.size()>2000:\n",
    "                train(q, q_target, memory, optimizer)\n",
    "        \n",
    "        if episode%print_interval==0 and episode!=0:\n",
    "            q_target.load_state_dict(q.state_dict())\n",
    "            print(\"n_episode : {}, Average Score : {:.1f}, Episode Score : {:.1f}, n_buffer : {}, eps : {:.1f}%\".format(\n",
    "                episode, score/episode, episode_score, memory.size(), epsilon*100))\n",
    "            \n",
    "        if episode%save_interval==0 and episode!=0:\n",
    "            # save model weights \n",
    "            torch.save(q_target.state_dict(), 'checkpoints/target_bot_%s.pt' % episode)\n",
    "            torch.save(q.state_dict(), 'checkpoints/policy_bot_%s.pt' % episode)\n",
    "    # save final model weights \n",
    "    torch.save(q_target.state_dict(), 'checkpoints/target_bot_final.pt')\n",
    "    torch.save(q.state_dict(), 'checkpoints/policy_bot_final.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b76d8cbffc149a7bc211b6349c3772b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_episode : 1, Average Score : -21.0, Episode Score : -21.0, n_buffer : 1014, eps : 8.0%\n",
      "n_episode : 2, Average Score : -21.0, Episode Score : -21.0, n_buffer : 2036, eps : 8.0%\n",
      "n_episode : 3, Average Score : -21.0, Episode Score : -21.0, n_buffer : 3638, eps : 8.0%\n",
      "n_episode : 4, Average Score : -21.0, Episode Score : -21.0, n_buffer : 4827, eps : 8.0%\n",
      "n_episode : 5, Average Score : -21.0, Episode Score : -21.0, n_buffer : 5925, eps : 8.0%\n",
      "n_episode : 6, Average Score : -21.0, Episode Score : -21.0, n_buffer : 7112, eps : 8.0%\n",
      "n_episode : 7, Average Score : -21.0, Episode Score : -21.0, n_buffer : 8951, eps : 8.0%\n",
      "n_episode : 8, Average Score : -21.0, Episode Score : -21.0, n_buffer : 10380, eps : 8.0%\n",
      "n_episode : 9, Average Score : -21.0, Episode Score : -21.0, n_buffer : 11894, eps : 8.0%\n",
      "n_episode : 10, Average Score : -21.0, Episode Score : -21.0, n_buffer : 13080, eps : 8.0%\n",
      "n_episode : 11, Average Score : -21.0, Episode Score : -21.0, n_buffer : 14423, eps : 7.9%\n",
      "n_episode : 12, Average Score : -21.0, Episode Score : -21.0, n_buffer : 15516, eps : 7.9%\n",
      "n_episode : 13, Average Score : -20.8, Episode Score : -19.0, n_buffer : 17030, eps : 7.9%\n",
      "n_episode : 14, Average Score : -20.9, Episode Score : -21.0, n_buffer : 18208, eps : 7.9%\n",
      "n_episode : 15, Average Score : -20.9, Episode Score : -21.0, n_buffer : 19645, eps : 7.9%\n",
      "n_episode : 16, Average Score : -20.9, Episode Score : -21.0, n_buffer : 20915, eps : 7.9%\n",
      "n_episode : 17, Average Score : -20.8, Episode Score : -20.0, n_buffer : 22374, eps : 7.9%\n",
      "n_episode : 18, Average Score : -20.8, Episode Score : -21.0, n_buffer : 23472, eps : 7.9%\n",
      "n_episode : 19, Average Score : -20.8, Episode Score : -21.0, n_buffer : 25044, eps : 7.9%\n",
      "n_episode : 20, Average Score : -20.9, Episode Score : -21.0, n_buffer : 26219, eps : 7.9%\n",
      "n_episode : 21, Average Score : -20.9, Episode Score : -21.0, n_buffer : 27385, eps : 7.9%\n",
      "n_episode : 22, Average Score : -20.9, Episode Score : -21.0, n_buffer : 28563, eps : 7.9%\n",
      "n_episode : 23, Average Score : -20.8, Episode Score : -20.0, n_buffer : 30022, eps : 7.9%\n",
      "n_episode : 24, Average Score : -20.8, Episode Score : -21.0, n_buffer : 31279, eps : 7.9%\n",
      "n_episode : 25, Average Score : -20.8, Episode Score : -20.0, n_buffer : 32599, eps : 7.9%\n",
      "n_episode : 26, Average Score : -20.8, Episode Score : -20.0, n_buffer : 33974, eps : 7.9%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-30674298a2b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-1deb022e4387>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(num_episodes)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                 \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mprint_interval\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-9f1eda32ed5b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(q, q_target, memory, optimizer)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# most reward we get in next state s_prime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mmax_q_prime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_prime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmax_q_prime\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdone_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# how much is our policy different from the true target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gym/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-4a4d1fd30380>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# action is either random or max probability estimated by Qnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gym/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gym/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gym/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gym/lib/python3.7/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gym/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    915\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Qnet:\n\tMissing key(s) in state_dict: \"head.0.weight\", \"head.0.bias\", \"head.2.weight\", \"head.2.bias\". \n\tUnexpected key(s) in state_dict: \"head.weight\", \"head.bias\". \n\tsize mismatch for conv1.weight: copying a param with shape torch.Size([16, 3, 5, 5]) from checkpoint, the shape in current model is torch.Size([32, 3, 8, 8]).\n\tsize mismatch for conv1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for bn1.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for bn1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for bn1.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for bn1.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for conv2.weight: copying a param with shape torch.Size([32, 16, 5, 5]) from checkpoint, the shape in current model is torch.Size([64, 32, 4, 4]).\n\tsize mismatch for conv2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for bn2.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for bn2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for bn2.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for bn2.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for conv3.weight: copying a param with shape torch.Size([32, 32, 5, 5]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for conv3.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for bn3.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for bn3.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for bn3.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for bn3.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-f9bdb1278827>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'checkpoints/target_bot_final.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gym/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    828\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 830\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    831\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Qnet:\n\tMissing key(s) in state_dict: \"head.0.weight\", \"head.0.bias\", \"head.2.weight\", \"head.2.bias\". \n\tUnexpected key(s) in state_dict: \"head.weight\", \"head.bias\". \n\tsize mismatch for conv1.weight: copying a param with shape torch.Size([16, 3, 5, 5]) from checkpoint, the shape in current model is torch.Size([32, 3, 8, 8]).\n\tsize mismatch for conv1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for bn1.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for bn1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for bn1.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for bn1.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for conv2.weight: copying a param with shape torch.Size([32, 16, 5, 5]) from checkpoint, the shape in current model is torch.Size([64, 32, 4, 4]).\n\tsize mismatch for conv2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for bn2.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for bn2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for bn2.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for bn2.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for conv3.weight: copying a param with shape torch.Size([32, 32, 5, 5]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for conv3.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for bn3.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for bn3.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for bn3.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for bn3.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64])."
     ]
    }
   ],
   "source": [
    "q = Qnet().to(device)\n",
    "q.load_state_dict(torch.load('checkpoints/target_bot_final.pt'))\n",
    "q.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record trained agent gameplay\n",
    "\n",
    "frames = []\n",
    "\n",
    "env.reset()\n",
    "current_s = get_screen()\n",
    "done = False\n",
    "last_s = get_screen()\n",
    "current_s = get_screen()\n",
    "s = last_s - current_s\n",
    "epsilon = 0.0\n",
    "while not done:\n",
    "    a = q.sample_action(s, epsilon) + 2\n",
    "    \n",
    "    # use environment's frame instead of preprocessed get_screen()\n",
    "    next_frame, _, done, info = env.step(a)\n",
    "    frames.append(next_frame)\n",
    "    last_s = current_s\n",
    "    current_s = get_screen()\n",
    "    s_prime = last_s - current_s\n",
    "\n",
    "    done_mask = 0.0 if done else 1.0\n",
    "    s = s_prime\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save game to video \n",
    "height, width = frames[0].shape[:2] \n",
    "\n",
    "writer = cv2.VideoWriter_fourcc('M','J','P','G')\n",
    "fps = 30\n",
    "video_file = 'playback.avi'\n",
    "out = cv2.VideoWriter(video_file, writer, fps, (width,height))\n",
    "for frame in frames:\n",
    "    out.write(frame)\n",
    "\n",
    "out.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
