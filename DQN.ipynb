{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import collections\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "env = gym.make('Pong-v0').unwrapped\n",
    "\n",
    "# set up matplotlib to open viewing window\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 160, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "def get_screen():\n",
    "    # convert to channel,h,w dimensions\n",
    "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "    \n",
    "    # erase background\n",
    "    screen[screen == 72] = 0 \n",
    "    screen[screen == 74] = 0 \n",
    "    screen[screen == 144] = 0 \n",
    "    screen[screen != 0] = 213\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    \n",
    "    screen = torch.from_numpy(screen)\n",
    "    \n",
    "    # convert to batch,channel,h,w dimensions\n",
    "    return resize(screen).unsqueeze(0).to(device)\n",
    "\n",
    "env.reset()\n",
    "# run game for a bit to load the ball and opponent paddle\n",
    "for i in range(50):\n",
    "    env.step(0)\n",
    "plt.figure()\n",
    "plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
    "           interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 52, 40])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_screen().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0005\n",
    "gamma = 0.98\n",
    "buffer_limit = 50000\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "store previous sequence-action pairs to decorrelate temporal locality\n",
    "transitions are composed of state, action, reward, next_state, and done_mask\n",
    "\"\"\"\n",
    "x_tmp = []\n",
    "class ReplayBuffer():\n",
    "    def __init__(self):\n",
    "        self.buffer = collections.deque(maxlen=buffer_limit)\n",
    "    \n",
    "    def put(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "        \n",
    "    def sample(self, n):\n",
    "        mini_batch = random.sample(self.buffer, n)\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
    "        \n",
    "        for transition in mini_batch:\n",
    "            s, a, r, s_prime, done_mask = transition\n",
    "            s_lst.append(s)\n",
    "            a_lst.append([a])\n",
    "            r_lst.append([r])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            done_mask_lst.append([done_mask])\n",
    "        \n",
    "        #print(s_lst)\n",
    "        #print(len(s_lst))\n",
    "        #print(type(a_lst))\n",
    "        #print(type(r_lst))\n",
    "        #print(type(s_prime_lst))\n",
    "        #print(type(done_mask_lst))\n",
    "        #x_tmp.append(s_lst)\n",
    "            \n",
    "        return  torch.cat(s_lst).to(device), \\\n",
    "                torch.tensor(a_lst).to(device), \\\n",
    "                torch.tensor(r_lst).to(device),\\\n",
    "                torch.cat(s_prime_lst).to(device), \\\n",
    "                torch.tensor(done_mask_lst).to(device)\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, h, w = get_screen().shape\n",
    "\n",
    "class Qnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Qnet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        self.head = nn.Linear(linear_input_size, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))\n",
    "    \n",
    "    # action is either random or max probability estimated by Qnet\n",
    "    def sample_action(self, obs, epsilon):\n",
    "        out = self.forward(obs) # don't need if random action \n",
    "        coin = random.random()\n",
    "        if coin < epsilon:\n",
    "            return random.randint(0,1)\n",
    "        else:\n",
    "            return out.argmax().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(q, q_target, memory, optimizer):\n",
    "    for i in range(10):\n",
    "        s,a,r,s_prime,done_mask = memory.sample(batch_size)\n",
    "        \n",
    "        q_out = q(s)\n",
    "        # collect output from the chosen action dimension\n",
    "        q_a = q_out.gather(1,a) \n",
    "        \n",
    "        # most reward we get in next state s_prime\n",
    "        max_q_prime = q_target(s_prime).max(1)[0].unsqueeze(1)\n",
    "        target = r + gamma * max_q_prime * done_mask\n",
    "        # how much is our policy different from the true target \n",
    "        loss = F.smooth_l1_loss(q_a, target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(num_episodes):\n",
    "    env = gym.make('Pong-v0')\n",
    "    q = Qnet().to(device)\n",
    "    q_target = Qnet().to(device)\n",
    "    q_target.load_state_dict(q.state_dict())\n",
    "    memory = ReplayBuffer()\n",
    "    \n",
    "    print_interval = 20\n",
    "    score = 0.0\n",
    "    optimizer = optim.Adam(q.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        # anneal 8% to 1% over training\n",
    "        epsilon = max(0.01, 0.08 - 0.01*(episode/200))\n",
    "        env.reset()\n",
    "        current_s = get_screen()\n",
    "        done = False\n",
    "        last_s = get_screen()\n",
    "        current_s = get_screen()\n",
    "        s = last_s - current_s\n",
    "        \n",
    "        while not done:\n",
    "            a = q.sample_action(s, epsilon)\n",
    "            # first variable would be s_prime but we have get_screen\n",
    "            _, r, done, info = env.step(a)\n",
    "            last_s = current_s\n",
    "            current_s = get_screen()\n",
    "            s_prime = last_s - current_s\n",
    "            \n",
    "            done_mask = 0.0 if done else 1.0\n",
    "            memory.put((s,a,r/100.,s_prime,done_mask))\n",
    "            s = s_prime\n",
    "            \n",
    "            score += r\n",
    "            if done:\n",
    "                break\n",
    "        if memory.size()>2000:\n",
    "            train(q, q_target, memory, optimizer)\n",
    "        \n",
    "        if episode%print_interval==0 and episode!=0:\n",
    "            q_target.load_state_dict(q.state_dict())\n",
    "            print(\"n_episode :{}, score : {:.1f}, n_buffer : {}, eps : {:.1f}%\".format(\n",
    "                episode, score/print_interval, memory.size(), epsilon*100))\n",
    "    # save final model weights \n",
    "    torch.save(q_target.state_dict(), 'target_bot.pt')\n",
    "    torch.save(q.state_dict(), 'policy_bot.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qnet(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(2, 2))\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(2, 2))\n",
       "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(2, 2))\n",
       "  (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (head): Linear(in_features=192, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = Qnet()\n",
    "q.load_state_dict(torch.load('target_bot.pt'))\n",
    "q.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record trained agent gameplay\n",
    "\n",
    "frames = []\n",
    "\n",
    "env.reset()\n",
    "current_s = get_screen()\n",
    "done = False\n",
    "last_s = get_screen()\n",
    "current_s = get_screen()\n",
    "s = last_s - current_s\n",
    "epsilon = 0.0\n",
    "while not done:\n",
    "    a = q.sample_action(s, epsilon)\n",
    "    \n",
    "    # use environment's frame instead of preprocessed get_screen()\n",
    "    next_frame, _, done, info = env.step(a)\n",
    "    frames.append(next_frame)\n",
    "    last_s = current_s\n",
    "    current_s = get_screen()\n",
    "    s_prime = last_s - current_s\n",
    "\n",
    "    done_mask = 0.0 if done else 1.0\n",
    "    s = s_prime\n",
    "\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save game to video \n",
    "height, width = frames[0].shape[:2] \n",
    "\n",
    "writer = cv2.VideoWriter_fourcc('M','J','P','G')\n",
    "fps = 30\n",
    "video_file = 'playback.avi'\n",
    "out = cv2.VideoWriter(video_file, writer, fps, (width,height))\n",
    "for frame in frames:\n",
    "    out.write(frame)\n",
    "\n",
    "out.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
